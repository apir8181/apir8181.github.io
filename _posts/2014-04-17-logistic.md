---
title: Logistic Regression
category: ml
---

**本文从kazenoyumechen.wordpress.com上迁移过来**  
_本文是一篇未完成的博文_

---


Logistic Regression在现实中是有效解决分类问题的分类算法之一。这个算法是处理二值分类问题的一个有效算法。二值分类问题指的是，数据集中的标签y只能取值为0或1。

按常理来说Linear Rgression也能应用到二值分类问题中，在这种情况下，当$$h_\theta(x) \geq 0.5$$，$$y = 1$$，当$$h_\theta(x) \lt 0.5$$, $$y = 0$$。但是由于Linear Regression的特点，异常值的出现会严重影响其分类效果，所以Linear Regression并不太适合于用作分类问题。

现有训练集，$$(y, x^{(i)})$$，其中$$y \in \{0,1\}$$，$$x^{(i)} \in \mathbb{R}^n$$，在Logistic Regression中，估值函数$$h_\theta(x) = \frac{1}{1 + e^{-\theta^{T}x^{(i)}}}$$。由估值函数定义域可知，其值域$$\mathbf{R} = [0, 1]$$。该$$h(x)$$函数图像如下图所示：  
![figure_1](/assets/posts/2014-04-17-logistic/figure_1.png)

同时，在Logistic Regression中，定义Cost Function:  
![figure_2](/assets/posts/2014-04-17-logistic/figure_2.png)

从Cost Function定义可看出，当$latex h_{\theta}(x)$分类越不确定时，CostFunction的值越大。同时定义目标函数  
$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m}(-y^{(i)}\log{(h_{\theta}(x^{(i)}))} - (1-y^{(i)})\log{(1 - h_{\theta}(x^{(i)}))}) $$  
求解该分类问题，实质上就是使得$$J(\theta)$$最小，又因  
$$\frac{d^2{(J(\theta))}}{d\theta_j^2} = \frac{1}{m} \sum_{i=1}^{m} {x_j^{(i)}}^2 h_{\theta}{(x^{(i)})} (1 - h_{\theta}(x^{(i)}) ) \geq 0$$

那么$$ J(\theta) $$是凸函数，因此，该等式可用gradient descent解决。  
$$ \theta_j = \theta_j - \alpha \sum_{i=1}^m [ h_{\theta}( x^{(i)} ) - y^{(i)} ] x_j^{(i)} $$
