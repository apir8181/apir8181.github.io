<html>
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8">
    <title>Onn</title>
    <link rel="stylesheet" type="text/css" media="screen" charset="utf-8" href="/assets/css/bootstrap.css"/>
    <link rel="stylesheet" type="text/css" media="screen" charset="utf-8" href="/assets/css/simple-sidebar.css"/>
    <link rel="stylesheet" type="text/css" media="screen" charset="utf-8" href="/assets/css/post.css"/>
    <link rel="stylesheet" type="text/css" media="screen" charset="utf-8" href="/assets/css/hemisu-light.css"/>
    <script src="/assets/js/jquery-1.11.1.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="/assets/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="/assets/js/google-code-prettify/prettify.js" type="text/javascript" charset="utf-8"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" charset="utf-8"></script>
    <script src="/assets/js/post.js" type="text/javascript" charset="utf-8"></script>
  </head>

  <body>
    <div id="wrapper">

      <div id="sidebar-wrapper">
        <ul class="sidebar-nav">
          <li class="sidebar-brand"><a href="/">Onn</a></li>
          <li>Life is short.</li>
          <li>Pursuing what you want.</li>
          <li><a href="/life.html">Life</a></li>
          <li><a href="/algorithm.html">Algorithm</a></li>
          <li><a href="/ml.html">Machine Learning</a></li>
          <li><a href="/system.html">System</a></li>
          <li><a href="/about.html">About</a></li>
        </ul>

      </div> 

      <div id="page-content-wrapper" class="container">
        <div id="page-content" class="col-lg-10 col-lg-offset-1 col-md-10 col-md-offset-1">
          <div>
  <h2 class="post-title">RDD结构及其操作 - Spark源代码学习</h2>
  <p class="post-date">22 October 2014</p>
  <div class="post-content"><p>本文假定读者看过Matei Zaharia的论文<a href="http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf">http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf</a>，熟悉spark术语并且有一定的spark使用经验。</p>

<p>RDD(Resilient Distributed Dataset)，是分布的只读数据集合。它是Spark中对数据集合的一种抽象，Spark对其定义了一系列的操作(map, filter, join等)。</p>

<p>以下将列出在spark源代码RDD类中，比较重要的数据及其操作。逻辑操作(map, reduce, join等)将会在下个专题中分析。</p>

<hr />

<p><strong>构造函数</strong>: 其中sc为Spark运行环境, deps为RDD的依赖。下文会详细说明。</p>

<pre class="prettyprint"><code>abstract class RDD[T: ClassTag](
    @transient private var sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
) extends Serializable with Logging { ... }

/** Construct an RDD with just a one-to-one dependency on one parent */
def this(@transient oneParent: RDD[_]) =
    this(oneParent.context , List(new OneToOneDependency(oneParent)))
</code></pre>

<hr />

<p><strong>sc</strong>: SparkContext的简称，表示这个RDD运行的Spark环境。</p>

<pre class="prettyprint"><code>/** The SparkContext that created this RDD. */
def sparkContext: SparkContext = sc
</code></pre>

<hr />

<p><strong>id</strong>: 在sc环境下这个RDD所拥有的唯一标识符。</p>

<pre class="prettyprint"><code>/** A unique ID for this RDD (within its SparkContext). */
val id: Int = sc.newRddId()
</code></pre>

<hr />

<p><strong>partitions</strong>: 代表RDD中部分数据的单元为partition。一个RDD由多个Partition组成，这些所有partition构成了RDD中的所有数据。checkpoint会在下文中说明。</p>

<pre class="prettyprint"><code>/**
 * Implemented by subclasses to return the set of partitions in this RDD. This method will only
 * be called once, so it is safe to implement a time-consuming computation in it.
 */
protected def getPartitions: Array[Partition]
    
@transient private var partitions_ : Array[Partition] = null
/**
 * Get the array of partitions of this RDD, taking into account whether the
 * RDD is checkpointed or not.
 */
 final def partitions: Array[Partition] = {
     checkpointRDD.map(_.partitions).getOrElse {
         if (partitions_ == null) {
         partitions_ = getPartitions
         }
         partitions_
     }
 }
</code></pre>

<hr />

<p><strong>partitioner</strong>: 一个RDD需要以特定的映射方式将数据分散到不同地方去，表示这个映射方式的正是partitioner。有两种Partitioner，HashPartitioner(默认)和RangePartitioner。</p>

<pre class="prettyprint"><code>/** Optionally overridden by subclasses to specify how they are partitioned. */
@transient val partitioner: Option[Partitioner] = None
</code></pre>

<hr />

<p><strong>dependencies</strong>: 一个RDD<script type="math/tex">T</script>通常由另外一个或多个RDD<script type="math/tex">(S_1, ..., S_i)</script>生成而来，会产生<script type="math/tex">(S_1, ..., S_i) \to T</script>的映射。在Spark中，dependencies正是表示这个产生这个映射的元组<script type="math/tex">(S_1, ..., S_i)</script>。</p>

<pre class="prettyprint"><code>/**
 * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only
 * be called once, so it is safe to implement a time-consuming computation in it.
 */
protected def getDependencies: Seq[Dependency[_]] = deps

private var dependencies_ : Seq[Dependency[_]] = null

/**
 * Get the list of dependencies of this RDD, taking into account whether the
 * RDD is checkpointed or not.
 */
final def dependencies: Seq[Dependency[_]] = {
    checkpointRDD.map(r =&gt; List(new OneToOneDependency(r))).getOrElse {
        if (dependencies_ == null) {
            dependencies_ = getDependencies
        }
        dependencies_
    }
}
</code></pre>

<hr />

<p><strong>storageLevel</strong>: 表示这个RDD的持久化方式，有不存储、存储到内存、存储到硬盘以及内存硬盘混合存储方式。默认情况下，数据是不进行存储的。persist()操作能改变这个RDD的持久化方式，cache()将这个RDD持久化到内存中，unpersist()能撤销这个RDD的持久化。</p>

<pre class="prettyprint"><code>private var storageLevel: StorageLevel = StorageLevel.NONE

/** Get the RDD's current storage level, or StorageLevel.NONE if none is set. */
def getStorageLevel = storageLevel
/**
 * Set this RDD's storage level to persist its values across operations after the first time
 * it is computed. This can only be used to assign a new storage level if the RDD does not
 * have a storage level set yet..
 */
def persist(newLevel: StorageLevel): this.type = {
    // TODO: Handle changes of StorageLevel
    if (storageLevel != StorageLevel.NONE &amp;&amp; newLevel != storageLevel) {
        throw new UnsupportedOperationException(
                "Cannot change storage level of an RDD after it was already assigned a level")
    }
    sc.persistRDD(this)
    // Register the RDD with the ContextCleaner for automatic GC-based cleanup
    sc.cleaner.foreach(_.registerRDDForCleanup(this))
    storageLevel = newLevel
    this
}

/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */
def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)

/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */
def cache(): this.type = persist()


/**
 * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.
 *
 * @param blocking Whether to block until all blocks are deleted.
 * @return This RDD.
 */
def unpersist(blocking: Boolean = true): this.type = {
    logInfo("Removing RDD " + id + " from persistence list")
    sc.unpersistRDD(id, blocking)
    storageLevel = StorageLevel.NONE
    this
}
</code></pre>

<p><strong>checkpoint</strong>: RDD可以通过调用checkpoint()的方法，将数据保存在节点硬盘或HDFS中，以应对数据丢失的情况。调用后，因为这个RDD已被存储在永久存储介质中，当需要这个RDD时并不需要重新进行计算，因此会将该RDD的dependencies移除。</p>

<pre class="prettyprint"><code>private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None

/**
 * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
 * directory set with SparkContext.setCheckpointDir() and all references to its parent
 * RDDs will be removed. This function must be called before any job has been
 * executed on this RDD. It is strongly recommended that this RDD is persisted in
 * memory, otherwise saving it on a file will require recomputation.
 */
def checkpoint() {
    if (context.checkpointDir.isEmpty) {
        throw new Exception("Checkpoint directory has not been set in the SparkContext")
    } else if (checkpointData.isEmpty) {
        checkpointData = Some(new RDDCheckpointData(this))
        checkpointData.get.markForCheckpoint()
    }
}

/** An Option holding our checkpoint RDD, if we are checkpointed */
private def checkpointRDD: Option[RDD[T]] = checkpointData.flatMap(_.checkpointRDD)

/**
 * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.
 */
private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
{
    if (isCheckpointed) firstParent[T].iterator(split, context) else compute(split, context)
}

/**
 * Return whether this RDD has been checkpointed or not
 */
def isCheckpointed: Boolean = {
    checkpointData.map(_.isCheckpointed).getOrElse(false)
}

/**
 * Gets the name of the file to which this RDD was checkpointed
 */
def getCheckpointFile: Option[String] = {
    checkpointData.flatMap(_.getCheckpointFile)
}
</code></pre>

<hr />

<p><strong>compute</strong>：获得这个RDD的某个partition的数据迭代器，split代表某个partition。</p>

<pre class="prettyprint"><code>/**
 * :: DeveloperApi ::
 * Implemented by subclasses to compute a given partition.
 */
@DeveloperApi
def compute(split: Partition, context: TaskContext): Iterator[T]

/**
 * Internal method to this RDD; will read from cache if applicable, or otherwise compute it.
 * This should ''not'' be called by users directly, but is available for implementors of custom
 * subclasses of RDD.
 */
final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
    if (storageLevel != StorageLevel.NONE) {
        SparkEnv.get.cacheManager.getOrCompute(this, split, context, storageLevel)
    } else {
        computeOrReadCheckpoint(split, context)
    }
}

/**
 * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.
 */
private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
{
    if (isCheckpointed) firstParent[T].iterator(split, context) else compute(split, context)
}
</code></pre>

<hr />

<p><strong>getPreferredLocations</strong>: 获得某个partition的偏好存储位置。例如对于操作HDFS块的RDD，这个操作将会返回块的存储位置。</p>

<pre class="prettyprint"><code>/**
 * Optionally overridden by subclasses to specify placement preferences.
 */
protected def getPreferredLocations(split: Partition): Seq[String] = Nil
</code></pre>

<hr />
</div>

  <!-- 多说评论框 start -->
  <div style="margin-top: 100px" class="ds-thread" data-thread-key="/system/2014/10/22/rdd" data-title="RDD结构及其操作 - Spark源代码学习" data-url="/system/2014/10/22/rdd.html"></div>
  <!-- 多说评论框 end -->
  <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
  <script type="text/javascript">
    var duoshuoQuery = {short_name:"onn"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->
</div>

        </div>
      </div>

    </div>
  </body>
</html>
